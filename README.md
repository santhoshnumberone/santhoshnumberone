# ğŸ‘‹ Hi, I'm Santhosh â€” AI DevTool Specialist | llama.cpp Â· LangChain Â· RAG | Mac M1 LLM Optimizer

ğŸ§  I build privacy-first AI tools that run offline â€” even on 8GB RAM Apple Silicon.  
GGUF Â· LangChain Â· CLI/RAG pipelines. No cloud. No API. No compromises.

---

## ğŸ§© The Pain  
_Almost every dev wants to experiment with LLMs â€” but experimentation means trial and error.
And trial and error comes at a cost._

_With cloud APIs, those costs compound fast â€” every prompt, every test run, every misstep eats into time and money.
Latency, usage caps, and vendor lock-in only add more friction._

_Whether you're a solo builder, startup, or enterprise â€” **cost sensitivity is real**.  
And if you're working on a tight setup (like I was, with 8GB RAM and no GPU), it's not just inconvenient â€” it's a hard blocker to progress._

---

## ğŸ’¥ The Breaking Point  
Anyone building on an 8GB MacBook is bound to hit a wall fast.  
Every prototype turns into a budgeting decision.

And for those of us who care about privacy or need offline reliability, cloud APIs arenâ€™t just inconvenient â€” theyâ€™re a blocker.  
For learners like me, it wasnâ€™t just about building â€” it was about *getting started at all*.

---

## ğŸ› ï¸ The Build

_Harvey Specter once said:_  
> â€œWhen you're backed against the wall, break the goddamn thing down.â€

So I did.
I flipped the stack â€” **vocal to local** â€” and started building fully local, open-source LLM tools using:
- ğŸ”— **LangChain** (retrievers, prompts, agents, memory)
- ğŸ§  **FAISS** for vector search
- ğŸ¤— **Hugging Face Transformers + SentenceTransformers**
- ğŸ§© **llama.cpp** with 4-bit GGUF models (Mistral, Zephyr)  
- ğŸ’¡ Custom prompt logic, fallback flows, and user-driven CLI UX

My focus: building lean, reproducible, zero-API workflows â€” ideal for devs, tinkerers, and anyone building in bandwidth or cost-constrained environments.

---

## ğŸ“– Featured Series
### 13" 8GB MacBook M1 pro. No Cloud.No API. Decent speed. Usable LLM On Zero Budget?

Running usable LLMs locally â€” on just an 8GB MacBook M1 Pro â€” without APIs, GPUs, or cloud credits?

I'm writing a 4-part Medium series that shares the full journey:
- What breaks, what works, and how far you can push llama.cpp on low RAM
- Models tested: Mistral, Phi, TinyLlama, Zephyr
- Benchmarks, thread tuning, quant formats, and real-world tradeoffs

ğŸ§  Ideal for devs trying to build without burning their wallets or sending data to someone elseâ€™s server.

ğŸ”— **PART 1/4** [Can anything actually run LLMs offline â€” on just an 8GB MacBook M1 Pro?](https://medium.com/@santhoshnumber1/part-1-13-8gb-macbook-m1-pro-no-cloud-no-api-decent-speed-usable-llm-on-zero-budget-71a84485bfef)

ğŸ”— **PART 2/4** [llama.cpp is in the spotlight â€” it promises local LLMs. But how usable is it, really?](https://medium.com/@santhoshnumber1/part-2-4-13-8gb-macbook-m1-pro-no-cloud-no-api-decent-speed-usable-llm-on-zero-budget-48e1837468d8)

ğŸ”— **PART 3/4** [Phi-3-mini takes center stage after Part 2 â€” now letâ€™s get into the nitty-gritty.](https://medium.com/@santhoshnumber1/part-3-4-13-8gb-macbook-m1-pro-no-cloud-no-api-decent-speed-usable-llm-on-zero-budget-635a8bf30efc)

ğŸ”— **PART 4/4** [From Part 2 & 3: llama.cpp(cli) + phi-3-mini = a powerful local LLM â€” soâ€¦ can we make it scale?](https://medium.com/@santhoshnumber1/part-4-4-13-8gb-macbook-m1-pro-no-cloud-no-api-decent-speed-usable-llm-on-zero-budget-e5d54b9e6746)

---

## ğŸ” The Insight  
Local-first LLMs give you full control over reliability, iteration speed, and customization.  
They shift AI from **a rented service** to **a tool you actually own** â€” and most importantly,  
they **bring the cost down to zero**.

Thatâ€™s what excites me.

---

## ğŸ“Š The Proof  
| Project | Purpose |
|--------|---------|
| `llm-power-search` | âœ… Local RAG pipeline that answers legal questions about open-source licenses using LangChain + FAISS + llama.cpp |
| *[Running Mistral 7B Locally on MacBook M1 Pro: Benchmarking Llama.cpp Python Inference Speed and GPU Trade-offs](https://medium.com/@santhoshnumber1/running-mistral-7b-locally-on-macbook-m1-pro-benchmarking-llama-cpp-89631f6c04b6)* | ğŸ“ˆ Performance comparison of 4-bit models on Mac M1 using llama.cpp, including speed vs GPU benchmarks |

More tools and ideas in progress â€” and Iâ€™m just getting started. ğŸ˜„

---

## ğŸ§  Tech Stack

- ğŸ”— LangChain Â· FAISS Â· SentenceTransformers  
- ğŸ§© llama.cpp Â· Hugging Face Â· GGUF 4-bit models  
- âš™ï¸ Python Â· CLI tooling Â· Local inference pipelines  
- ğŸ§ª PyTorch Â· TensorFlow (CV/ML background)  
- ğŸ§° C++ (Gtkmm), Python (PyQt/OpenCV) for earlier UI systems

---

## ğŸ” In Short  
ğŸ§  I specialize in local-first LLM devtools â€” built for privacy, reproducibility, and edge performance.

If you're building something that needs:
- âœ… Full offline support
- âœ… Reliable RAG pipelines on low-spec devices
- âœ… Streamlit/CLI/PyQt interfaces for local AI
- âœ… Mac M1/M2 performance optimization for LLMs

---

## ğŸš€ Iâ€™m Open To:
- âœ… Remote roles in **LLM prototyping** or **AI devtools**  
- âœ… **AI Product Management** roles focused on user-first GenAI tools  
- âœ… OSS / SaaS collabs with a focus on usability, cost-efficiency, and impact  

ğŸ“© <santhoshnumber1@gmail.com>  
ğŸ”— [LinkedIn â†’](https://www.linkedin.com/in/santhosh-electraanu/)

---

## ğŸ“ Learning & Certifications

- [âœ”ï¸ Prompt Engineering for ChatGPT (Coursera)](https://coursera.org/share/7197a7bd0ae717ecced1ed917a54f3e8)  
- [âœ”ï¸ Trustworthy Generative AI (Vanderbilt)](https://coursera.org/share/6c5944df9f15f37a9082aebf20d7ca6a)  
- [âœ”ï¸ ChatGPT Advanced Data Analysis (Vanderbilt)](https://coursera.org/share/8ae368d556e85dcf809a107b823d212d) 
- ğŸ§  LangChain Dev Course (DeepLearning.AI)  
- ğŸ”¬ ChatGPT Prompt Engineering for Developers (OpenAI)

---

## ğŸ” My Journey So Far

### ğŸ“ Where I Started  
I began my career as a Computer Vision developer â€” building tools that combined low-level image processing with product intuition.

Projects included:
- ğŸ¥” **Size & colorâ€“based potato sorting system** â€” image processing algorithm deployed via Google Cloud Functions
- ğŸ§ª **[Custom designed CNN trained from scratch](https://github.com/santhoshnumberone/Image-Forgery-Detection-)** on a local machine for spliced image forgery detection (600+ epochs) [training loss](https://plotly.com/~santhoshnumberone/9/#/) & [training accuracy](https://plotly.com/~santhoshnumberone/11/#/)
- ğŸ‘ï¸ **Early glaucoma detection prototype** â€” built on Raspberry Pi with OpenCV + VR headset integration
- ğŸš— **Real-time vehicle flow analysis** â€” 24-hour video inference across lanes on AWS servers using YOLO
- ğŸ§° **Internal OpenCV tool replication** â€” led a team replicating a core analytics tool for reuse
- ğŸ§‘â€ğŸ’» **Full UI/UX design** for embedded systems â€” owned v1 + v2 flow for industrial machine vision tool

---

### ğŸ”„ Where I Am Now  
From the start, I've owned not just features â€” but the full flow: 
`problem` â†’ `interface` â†’ `model` â†’ `deployment`.
That mindset now drives my transition into:
- âœ… **LLM prototyping**
- âœ… **Offline AI tooling**
- âœ… **End-to-end product thinking**

What began as an **offline learning constraint** turned out to be a **blessing** â€” forcing me to focus on **privacy**, **full ownership**, and **infinite iteration** where imagination was the only limit (and system RAM the only bottleneck). 

That journey led to **zero-cost**, local-first tools that work for **solo devs**, **startups**, and **eventually even cost-sensitive enterprises**.

Itâ€™s no longer just about building features â€” Iâ€™m evolving into a product manager who takes full ownership, end to end.

---

> ğŸ§ª From a young boy who believed that â€” unlike most things in life â€” **code usually does exactly what you want**...  
to early repos here that might not mean much to others,  
but marked real milestones for me.  
And soon: tools that I hope will matter â€” not just to me, but to many of us building with constraints, creativity, and purpose.

